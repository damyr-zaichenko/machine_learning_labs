# -*- coding: utf-8 -*-
"""machine_learning_lab3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r4jBFPm6L-8NdBXBWb8bKrLOhHL4SFP0
"""

import pandas as pd

"""1. Відкрити та зчитати наданий файл з даними."""

df = pd.read_csv('dataset_2.txt', sep=',', header=None, names=[str(i) for i in range(8)])

"""2. Визначити та вивести кількість записів та кількість полів у
завантаженому наборі даних.
"""

print("{} записів, {} полів".format(*df.shape))

"""3. Вивести перші 10 записів набору даних."""

df.head(10)

"""4. Розділити набір даних на навчальну (тренувальну) та тестову вибірки."""

from sklearn.model_selection import train_test_split

X = df.iloc[:, 2:7]
y = df['7']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1444)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(max_depth=5)

clf.fit(X_train, y_train)

from sklearn.tree import export_graphviz
import graphviz
from IPython.display import Image
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import pydotplus

dot_data = export_graphviz(clf, out_file=None,
                feature_names=X_train.columns,
                class_names=['0', '1'],
                filled=True, rounded=True,
                special_characters=True)

graph = graphviz.Source(dot_data)

pydot_graph = pydotplus.graph_from_dot_data(dot_data)

graph.render("хуй", format='png', cleanup=True, view=True)

graph

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, matthews_corrcoef

metrics = ['accuracy_score', 'precision_score', 'recall_score', 'f1_score', 'balanced_accuracy_score', 'matthews_corrcoef']
metrics_labels = ['accuracy', 'precision', 'recall', 'f1 score', 'balanced_accuracy', 'mcc']

def calculate_metrics(y_true, y_pred, verbose=False, ret_res=True):
  result = []
  for metric in metrics:
    metric_value = eval(f'{metric}(y_true, y_pred)')
    if verbose: print(f"{metric} = {round(metric_value, 5)}")
    result.append(metric_value)
  if ret_res: return result

y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)

calculate_metrics(y_train, y_pred_train, verbose=True, ret_res=False)
print()
calculate_metrics(y_test, y_pred_test, verbose=True, ret_res=False)

fig, axs = plt.subplots(1, 2, figsize=(10, 5))

axs[0].scatter(metrics_labels, calculate_metrics(y_train, y_pred_train), label="Тренувальна вибірка")
axs[0].set_ylim((0.95, 1))
axs[0].set_xticklabels(metrics_labels, rotation=50)
axs[0].scatter(metrics_labels, calculate_metrics(y_test, y_pred_test), label="Тестова вибірка")
axs[0].legend()

entropy_clf = DecisionTreeClassifier(max_depth=5)
entropy_clf.fit(X_train, y_train)

ent_y_pred_train = entropy_clf.predict(X_train)
ent_y_pred_test = entropy_clf.predict(X_test)

axs[1].scatter(metrics_labels, calculate_metrics(y_train, ent_y_pred_train), label="Тренувальна вибірка")
axs[1].set_ylim((0.95, 1))
axs[1].set_xticklabels(metrics_labels, rotation=50)
axs[1].scatter(metrics_labels, calculate_metrics(y_test, ent_y_pred_test), label="Тестова вибірка")
axs[1].legend()
plt.show()

depth_results_train = pd.DataFrame(columns=metrics_labels)
depth_results_test = pd.DataFrame(columns=metrics_labels)

for max_leaf_nodes in range(2, 20):

  clf = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)
  clf.fit(X_train, y_train)

  y_pred = clf.predict(X_train)
  depth_results_train.loc[len(depth_results_train)] = calculate_metrics(y_train, y_pred)

  y_pred = clf.predict(X_test)
  depth_results_test.loc[len(depth_results_test)] = calculate_metrics(y_test, y_pred)

fig, axs = plt.subplots(1, 2, figsize=(10, 5))

depth_results_train.index += 2
depth_results_test.index += 2

depth_results_train.plot(ax=axs[0])
axs[0].set_title('Тренувальна вибірка')

depth_results_test.plot(ax=axs[1])
axs[1].set_title('Тестова вибірка')

plt.tight_layout()

plt.show()

depth_results_train = pd.DataFrame(columns=metrics_labels)
depth_results_test = pd.DataFrame(columns=metrics_labels)

for min_samples_leaf in range(1, 10):

  clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)
  clf.fit(X_train, y_train)

  y_pred = clf.predict(X_train)
  depth_results_train.loc[len(depth_results_train)] = calculate_metrics(y_train, y_pred)

  y_pred = clf.predict(X_test)
  depth_results_test.loc[len(depth_results_test)] = calculate_metrics(y_test, y_pred)

fig, axs = plt.subplots(1, 2, figsize=(10, 5))

depth_results_train.index += 2
depth_results_test.index += 2

depth_results_train.plot(ax=axs[0])
axs[0].set_title('Тренувальна вибірка')

depth_results_test.plot(ax=axs[1])
axs[1].set_title('Тестова вибірка')

plt.tight_layout()

plt.show()

importances = clf.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': range(len(importances)), 'Importance': importances})

feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importances')
plt.show()